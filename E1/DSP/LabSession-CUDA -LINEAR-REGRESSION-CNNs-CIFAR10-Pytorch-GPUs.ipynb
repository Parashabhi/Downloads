{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db66721",
   "metadata": {},
   "source": [
    "# CPU\n",
    "\n",
    "\n",
    "\n",
    "* A processor core is an individual processor within a Central Processing Unit (CPU). \n",
    "* This core is the computer chip inside the CPU that performs calculations. \n",
    "* Today nearly all computers have multi-core processors, which means that their CPU contains more than one core. \n",
    "* This increases the performance of the CPU because it can do more calculations. \n",
    "* Confusingly, the terms processor and core often get used interchangeably.\n",
    "\n",
    "# then what is a GPU?\n",
    "\n",
    "\n",
    "* Graphics Processing Units (GPUs) are specialised processors that contain many cores. \n",
    "* They were originally designed to render graphics but these days they’re used for other things as well. \n",
    "* Although CPUs can have multiple cores they don’t have nearly as many as a GPU. \n",
    "* Typically GPUs will have 1000s of small cores on a single processor. \n",
    "* The differences between CPUs and GPUs are summarised in this table.\n",
    "![CPU vs GPU](cpugpu_comp.png)\n",
    "\n",
    "\n",
    "\n",
    "It’s important to note that just because GPUs have more cores than CPUs they are not universally better. There is a trade-off between increasing the number of cores and the flexibility of what they can be used for. GPU cores are smaller and more specialised than the cores on a CPU, this means that they are better for specific applications, but cannot be optimised or used efficiently in as many ways as CPUs.\n",
    "\n",
    "In particular GPUs are very efficient for performing highly parallel matrix multiplication, because this is an important application for graphics rendering.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7258b0a",
   "metadata": {},
   "source": [
    "# CUDA vs. OpenCL\n",
    "The Compute Unified Device Architecture (CUDA) is a parallel computing platform developed by NVIDIA that enables software to jointly use both the CPU and GPU. OpenCL (Open Computing Language) is an alternative and more general parallel computing platform developed by Apple that allows software to access not only CPU and GPU simultaneously, but also FPGAs (field programmable gate arrays) and other DSP (digital signal processors). Both CUDA and OpenCL are compatible with Python and allow the user to implement highly parallel processes on a GPU without needing to explicitly specify the parallelisation, i.e. they do the optimisation for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ba560",
   "metadata": {},
   "source": [
    "# AMD vs. NVIDIA\n",
    "While both AMD and NVIDIA are major vendors of GPUs, NVIDIA is currently the most common GPU vendor for machine learning and cloud computing. Most GPU-enabled Python libraries will only work with NVIDIA GPUs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197e7c5",
   "metadata": {},
   "source": [
    "# Different types of GPU\n",
    "This is a comparison of some of the most widely used NVIDIA GPUs in terms of their core numbers and memory. There are of course other differences between these GPUs (including price!) which may lead to a specific type being chosen for a particular computing centre or application, but they are all suitable for AI.\n",
    "![GPUs](gpus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234e405",
   "metadata": {},
   "source": [
    "# Which Python machine learning libraries support GPUs?\n",
    "All of the major deep learning Python libraries support the use of GPUs and allow users to distribute their code over multiple GPUs.\n",
    "\n",
    "![sGPUs](sgpus.png)\n",
    "An important ML Python library that you may notice is missing from this list is Scikit-Learn. Scikit-learn does not support GPU processing at the moment and there are currently no plans to implement support in the near future. Why is this? Well, GPU suport is primarily used for neural networks and deep learning, neither of which are key elements of the Scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae20a7c",
   "metadata": {},
   "source": [
    "# Why should I use a GPU for my ML code?\n",
    "The matrix operations that GPus are optimised for are exactly what happens in the training step for building a deep learning model. In a neural network, the process of multiplying input data by weights can be formulated as a matrix operation and as your network grows to include 10s of millions of parameters it also becomes a pretty big one. Having many cores available to perform this matrix multiplication in parallel means that the GPU can quickly outperform a CPU in this case.\n",
    "\n",
    "However, if you’re not using a neural network as your machine learning model you may find that a GPU doesn’t improve the computation time. It’s the large matrix multiplications required for neural networks that really make GPUs useful. Likewise if you are using a neural network but its very small then again a GPU will not be any faster than a CPU - in fact it might even be slower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cc079",
   "metadata": {},
   "source": [
    "# Find out if a GPU is available\n",
    "The first thing you need to know when you’re thinking of using a GPU is whether there is actually one available. For PyTorch this can be done using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97243901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out if a GPU is available\n",
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8383b78",
   "metadata": {},
   "source": [
    "# Find out the specifications of the GPU(s)\n",
    "There are a wide variety of GPUs available these days, so it’s often useful to check the specifications of the GPU(s) that are available to you. For example, the following lines of code will tell you\n",
    "*  which version of CUDA the GPU(s) support \n",
    "*  how many GPUs there are available \n",
    "*  for a specific GPU (here 0) what kind of GPU it is, and \n",
    "*  how much memory it has available in total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3962e08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf63960",
   "metadata": {},
   "source": [
    "# CPU Equivalent\n",
    "If you want to do the same for your CPU from Python you can find out what it is using:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "platform.processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7700d4",
   "metadata": {},
   "source": [
    "# Selecting a GPU to use\n",
    "In PyTorch, you can use the use_cuda flag to specify which device you want to use. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f670f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6d53f",
   "metadata": {},
   "source": [
    "will set the device to the GPU if one is available and to the CPU if there isn’t a GPU available. This means that you don’t need to hard code changes into your code to use one or the other. If there are multiple GPUs available then you can specify a particular GPU using its index, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ca950",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if use_cuda else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ef115",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "At its core, PyTorch is a library for processing tensors. A tensor is a number, vector, matrix, or any n-dimensional array. Let's create a tensor with a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number\n",
    "import torch\n",
    "t1 = torch.tensor(4.)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ce45a",
   "metadata": {},
   "source": [
    "`4.` is a shorthand for `4.0`. It is used to indicate to Python (and PyTorch) that you want to create a floating-point number. We can verify this by checking the `dtype` attribute of our tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aeacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ac1f5",
   "metadata": {},
   "source": [
    "Let's try creating more complex tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b11123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector\n",
    "t2 = torch.tensor([1., 2, 3, 4])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b86ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix\n",
    "t3 = torch.tensor([[5., 6], \n",
    "                   [7, 8], \n",
    "                   [9, 10]])\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-dimensional array\n",
    "t4 = torch.tensor([\n",
    "    [[11, 12, 13], \n",
    "     [13, 14, 15]], \n",
    "    [[15, 16, 17], \n",
    "     [17, 18, 19.]]])\n",
    "t4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf44dd",
   "metadata": {},
   "source": [
    "Tensors can have any number of dimensions and different lengths along each dimension. We can inspect the length along each dimension using the `.shape` property of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1706570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1)\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d1b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t2)\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e8a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(t3)\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e9c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(t4)\n",
    "t4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5773f4",
   "metadata": {},
   "source": [
    "## Tensor operations and gradients\n",
    "\n",
    "We can combine tensors with the usual arithmetic operations. Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42833394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors.\n",
    "import torch\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d01465",
   "metadata": {},
   "source": [
    "We've created three tensors: `x`, `w`, and `b`, all numbers. `w` and `b` have an additional parameter `requires_grad` set to `True`. We'll see what it does in just a moment. \n",
    "\n",
    "Let's create a new tensor `y` by combining these tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027343c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Arithmetic operations\n",
    "y = torch.tensor(0., requires_grad=True)\n",
    "y = w * x + b\n",
    "print(y)\n",
    "actual = 20\n",
    "error = actual  - y\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb1620b",
   "metadata": {},
   "source": [
    "As expected, `y` is a tensor with the value `3 * 4 + 5 = 17`. What makes PyTorch unique is that we can automatically compute the derivative of `y` w.r.t. the tensors that have `requires_grad` set to `True` i.e. w and b. This feature of PyTorch is called _autograd_ (automatic gradients).\n",
    "\n",
    "To compute the derivatives, we can invoke the `.backward` method on our result `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e182c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives\n",
    "error.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1514a5",
   "metadata": {},
   "source": [
    "The derivatives of `y` with respect to the input tensors are stored in the `.grad` property of the respective tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9c742",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display gradients\n",
    "print('dy/dx:', x.grad)\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec058d67",
   "metadata": {},
   "source": [
    "As expected, `dy/dw` has the same value as `x`, i.e., `3`, and `dy/db` has the value `1`. Note that `x.grad` is `None` because `x` doesn't have `requires_grad` set to `True`. \n",
    "\n",
    "The \"grad\" in `w.grad` is short for _gradient_, which is another term for derivative. The term _gradient_ is primarily used while dealing with vectors and matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84db78",
   "metadata": {},
   "source": [
    "## Tensor functions\n",
    "\n",
    "Apart from arithmetic operations, the `torch` module also contains many functions for creating and manipulating tensors. Let's look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06afbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with a fixed value for every element\n",
    "t6 = torch.full((3, 2), 0)\n",
    "t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d3782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate two tensors with compatible shapes\n",
    "print(t3)\n",
    "t7 = torch.cat((t3, t6))\n",
    "t7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b571e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the sin of each element\n",
    "t8 = torch.sin(t7)\n",
    "t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727130b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the shape of a tensor\n",
    "t9 = t8.reshape(3, 2, 2)\n",
    "t9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e87e0",
   "metadata": {},
   "source": [
    "You can learn more about tensor operations here: https://pytorch.org/docs/stable/torch.html . Experiment with some more tensor functions and operations using the empty cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782cbbe",
   "metadata": {},
   "source": [
    "## Interoperability with Numpy\n",
    "\n",
    "[Numpy](http://www.numpy.org/) is a popular open-source library used for mathematical and scientific computing in Python. It enables efficient operations on large multi-dimensional arrays and has a vast ecosystem of supporting libraries, including:\n",
    "\n",
    "* [Pandas](https://pandas.pydata.org/) for file I/O and data analysis\n",
    "* [Matplotlib](https://matplotlib.org/) for plotting and visualization\n",
    "* [OpenCV](https://opencv.org/) for image and video processing\n",
    "\n",
    "\n",
    "If you're interested in learning more about Numpy and other data science libraries in Python, check out this tutorial series: https://jovian.ai/aakashns/python-numerical-computing-with-numpy .\n",
    "\n",
    "Instead of reinventing the wheel, PyTorch interoperates well with Numpy to leverage its existing ecosystem of tools and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7f8c6",
   "metadata": {},
   "source": [
    "Here's how we create an array in Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91360293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2], [3, 4.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38009100",
   "metadata": {},
   "source": [
    "We can convert a Numpy array to a PyTorch tensor using `torch.from_numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedeb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11943148",
   "metadata": {},
   "source": [
    "Let's verify that the numpy array and torch tensor have similar data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f587b5",
   "metadata": {},
   "source": [
    "We can convert a PyTorch tensor to a Numpy array using the `.numpy` method of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a torch tensor to a numpy array\n",
    "z = y.numpy()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb547b3",
   "metadata": {},
   "source": [
    "The interoperability between PyTorch and Numpy is essential because most datasets you'll work with will likely be read and preprocessed as Numpy arrays.\n",
    "\n",
    "You might wonder why we need a library like PyTorch at all since Numpy already provides data structures and utilities for working with multi-dimensional numeric data. There are two main reasons:\n",
    "\n",
    "1. **Autograd**: The ability to automatically compute gradients for tensor operations is essential for training deep learning models.\n",
    "2. **GPU support**: While working with massive datasets and large models, PyTorch tensor operations can be performed efficiently using a Graphics Processing Unit (GPU). Computations that might typically take hours can be completed within minutes using GPUs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf190b96",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "\n",
    "We'll create a model that predicts crop yields for apples and oranges (*target variables*) by looking at the average temperature, rainfall, and humidity (*input variables or features*) in a region. Here's the training data:\n",
    "\n",
    "![linear-regression-training-data](https://i.imgur.com/6Ujttb4.png)\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "```\n",
    "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "```\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "![linear-regression-graph](https://i.imgur.com/4DJ9f8X.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb3663",
   "metadata": {},
   "source": [
    "The *learning* part of linear regression is to figure out a set of weights `w11, w12,... w23, b1 & b2` using the training data, to make accurate predictions for new data. The _learned_ weights will be used to predict the yields for apples and oranges in a new region using the average temperature, rainfall, and humidity for that region. \n",
    "\n",
    "We'll _train_ our model by adjusting the weights slightly many times to make better predictions, using an optimization technique called *gradient descent*. Let's begin by importing Numpy and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc90934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aecfd4",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "We can represent the training data using two matrices: `inputs` and `targets`, each with one row per observation, and one column per variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8994b2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391eda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a9256c",
   "metadata": {},
   "source": [
    "We've separated the input and target variables because we'll operate on them separately. Also, we've created numpy arrays, because this is typically how you would work with training data: read some CSV files as numpy arrays, do some processing, and then convert them to PyTorch tensors.\n",
    "\n",
    "Let's convert the arrays to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a839f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfbab5",
   "metadata": {},
   "source": [
    "## Linear regression model from scratch\n",
    "\n",
    "The weights and biases (`w11, w12,... w23, b1 & b2`) can also be represented as matrices, initialized as random values. The first row of `w` and the first element of `b` are used to predict the first target variable, i.e., yield of apples, and similarly, the second for oranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed946953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0583, -0.9156,  0.8492],\n",
      "        [-0.4226, -0.6254, -0.2129]], requires_grad=True)\n",
      "tensor([ 0.2065, -1.3853], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3eba9",
   "metadata": {},
   "source": [
    "`torch.randn` creates a tensor with the given shape, with elements picked randomly from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with mean 0 and standard deviation 1.\n",
    "\n",
    "Our *model* is simply a function that performs a matrix multiplication of the `inputs` and the weights `w` (transposed) and adds the bias `b` (replicated for each observation).\n",
    "\n",
    "![matrix-mult](https://i.imgur.com/WGXLFvA.png)\n",
    "\n",
    "We can define the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d70ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd6dc0",
   "metadata": {},
   "source": [
    "`@` represents matrix multiplication in PyTorch, and the `.t` method returns the transpose of a tensor.\n",
    "\n",
    "The matrix obtained by passing the input data into the model is a set of predictions for the target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072cfe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  52.6301,  -83.2896],\n",
      "        [  70.2842, -108.5005],\n",
      "        [  18.8383, -134.2989],\n",
      "        [ 100.1994,  -79.2584],\n",
      "        [  44.7724, -105.4841]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1327340",
   "metadata": {},
   "source": [
    "Let's compare the predictions of our model with the actual targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d47338f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Compare with targets\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbc104",
   "metadata": {},
   "source": [
    "You can see a big difference between our model's predictions and the actual targets because we've initialized our model with random weights and biases. Obviously, we can't expect a randomly initialized model to *just work*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0fcb8",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Before we improve our model, we need a way to evaluate how well our model is performing. We can compare the model's predictions with the actual targets using the following method:\n",
    "\n",
    "* Calculate the difference between the two matrices (`preds` and `targets`).\n",
    "* Square all elements of the difference matrix to remove negative values.\n",
    "* Calculate the average of the elements in the resulting matrix.\n",
    "\n",
    "The result is a single number, known as the **mean squared error** (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a24a6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5aeb5f",
   "metadata": {},
   "source": [
    "`torch.sum` returns the sum of all the elements in a tensor. The `.numel` method of a tensor returns the number of elements in a tensor. Let's compute the mean squared error for the current predictions of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7113bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(22241.0117, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a713f66",
   "metadata": {},
   "source": [
    "Here’s how we can interpret the result: *On average, each element in the prediction differs from the actual target by the square root of the loss*. And that’s pretty bad, considering the numbers we are trying to predict are themselves in the range 50–200. The result is called the *loss* because it indicates how bad the model is at predicting the target variables. It represents information loss in the model: the lower the loss, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bc160",
   "metadata": {},
   "source": [
    "## Compute gradients\n",
    "\n",
    "With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases because they have `requires_grad` set to `True`. We'll see how this is useful in just a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a753931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eaa33b",
   "metadata": {},
   "source": [
    "The gradients are stored in the `.grad` property of the respective tensors. Note that the derivative of the loss w.r.t. the weights matrix is itself a matrix with the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32a672a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0583, -0.9156,  0.8492],\n",
      "        [-0.4226, -0.6254, -0.2129]], requires_grad=True)\n",
      "tensor([[ -1195.3145,  -3363.5432,  -1564.5299],\n",
      "        [-16171.4893, -18214.8164, -11103.6523]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82fb7a9",
   "metadata": {},
   "source": [
    "## Adjust weights and biases to reduce the loss\n",
    "\n",
    "The loss is a [quadratic function](https://en.wikipedia.org/wiki/Quadratic_function) of our weights and biases, and our objective is to find the set of weights where the loss is the lowest. If we plot a graph of the loss w.r.t any individual weight or bias element, it will look like the figure shown below. An important insight from calculus is that the gradient indicates the rate of change of the loss, i.e., the loss function's [slope](https://en.wikipedia.org/wiki/Slope) w.r.t. the weights and biases.\n",
    "\n",
    "If a gradient element is **positive**:\n",
    "\n",
    "* **increasing** the weight element's value slightly will **increase** the loss\n",
    "* **decreasing** the weight element's value slightly will **decrease** the loss\n",
    "\n",
    "![postive-gradient](https://i.imgur.com/WLzJ4xP.png)\n",
    "\n",
    "If a gradient element is **negative**:\n",
    "\n",
    "* **increasing** the weight element's value slightly will **decrease** the loss\n",
    "* **decreasing** the weight element's value slightly will **increase** the loss\n",
    "\n",
    "![negative=gradient](https://i.imgur.com/dvG2fxU.png)\n",
    "\n",
    "The increase or decrease in the loss by changing a weight element is proportional to the gradient of the loss w.r.t. that element. This observation forms the basis of _the gradient descent_ optimization algorithm that we'll use to improve our model (by _descending_ along the _gradient_).\n",
    "\n",
    "We can subtract from each weight element a small quantity proportional to the derivative of the loss w.r.t. that element to reduce the loss slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fa0f8",
   "metadata": {},
   "source": [
    "We multiply the gradients with a very small number (`10^-5` in this case) to ensure that we don't modify the weights by a very large amount. We want to take a small step in the downhill direction of the gradient, not a giant leap. This number is called the *learning rate* of the algorithm. \n",
    "\n",
    "We use `torch.no_grad` to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7422936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify that the loss is actually lower\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a2edc",
   "metadata": {},
   "source": [
    "Before we proceed, we reset the gradients to zero by invoking the `.zero_()` method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke `.backward` on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3845fe",
   "metadata": {},
   "source": [
    "## Train the model using gradient descent\n",
    "\n",
    "As seen above, we reduce the loss and improve our model using the gradient descent optimization algorithm. Thus, `we can _train_ the model using the following steps:\n",
    "\n",
    "1. Generate predictions\n",
    "\n",
    "2. Calculate the loss\n",
    "\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    "5. Reset the gradients to zero\n",
    "\n",
    "Let's implement the above step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccafec34",
   "metadata": {},
   "source": [
    "Let's update the weights and biases using the gradients computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de707038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights & reset gradients\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd60aa14",
   "metadata": {},
   "source": [
    "Let's take a look at the new weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c2de7",
   "metadata": {},
   "source": [
    "With the new weights and biases, the model should have a lower loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb88249",
   "metadata": {},
   "source": [
    "We have already achieved a significant reduction in the loss merely by adjusting the weights and biases slightly using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1db653",
   "metadata": {},
   "source": [
    "## Train for multiple epochs\n",
    "\n",
    "To reduce the loss further, we can repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an _epoch_. Let's train the model for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3197d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs\n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8f571",
   "metadata": {},
   "source": [
    "Once again, let's verify that the loss is now lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119fa00",
   "metadata": {},
   "source": [
    "The loss is now much lower than its initial value. Let's look at the model's predictions and compare them with the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5154dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec397545",
   "metadata": {},
   "source": [
    "The predictions are now quite close to the target variables. We can get even better results by training for a few more epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eafead8",
   "metadata": {},
   "source": [
    "## Linear regression using PyTorch built-ins\n",
    "\n",
    "We've implemented linear regression & gradient descent model using some basic tensor operations. However, since this is a common pattern in deep learning, PyTorch provides several built-in functions and classes to make it easy to create and train models with just a few lines of code.\n",
    "\n",
    "Let's begin by importing the `torch.nn` package from PyTorch, which contains utility classes for building neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44aa4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  torch --- nn \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d489ea",
   "metadata": {},
   "source": [
    "As before, we represent the inputs and targets and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcd00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6435a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0a3f6",
   "metadata": {},
   "source": [
    "We are using 15 training examples to illustrate how to work with large datasets in small batches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f83f7",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "\n",
    "We'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples, and provides standard APIs for working with many different types of datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training --- image --- b x c x h x w\n",
    "from torch.utils.data import TensorDataset\n",
    "#import torch.utils.data.TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f203921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d78a9",
   "metadata": {},
   "source": [
    "The `TensorDataset` allows us to access a small section of the training data using the array indexing notation (`[0:3]` in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015cb3f",
   "metadata": {},
   "source": [
    "We'll also create a `DataLoader`, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711624a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6f126",
   "metadata": {},
   "source": [
    "We can use the data loader in a `for` loop. Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b84e72",
   "metadata": {},
   "source": [
    "In each iteration, the data loader returns one batch of data with the given batch size. If `shuffle` is set to `True`, it shuffles the training data before creating batches. Shuffling helps randomize the input to the optimization algorithm, leading to a faster reduction in the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c76e1c",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "\n",
    "Instead of initializing the weights & biases manually, we can define the model using the `nn.Linear` class from PyTorch, which does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34291684",
   "metadata": {},
   "source": [
    "PyTorch models also have a helpful `.parameters` method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff792bb2",
   "metadata": {},
   "source": [
    "We can use the model to generate predictions in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f58ea2",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Instead of defining a loss function manually, we can use the built-in loss function `mse_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nn.functional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7123c",
   "metadata": {},
   "source": [
    "The `nn.functional` package contains many useful loss functions and several other utilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6096aa",
   "metadata": {},
   "source": [
    "Let's compute the loss for the current predictions of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e58483",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7620ff5",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Instead of manually manipulating the model's weights & biases using gradients, we can use the optimizer `optim.SGD`. SGD is short for \"stochastic gradient descent\". The term _stochastic_ indicates that samples are selected in random batches instead of as a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2185f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98761f6",
   "metadata": {},
   "source": [
    "Note that `model.parameters()` is passed as an argument to `optim.SGD` so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate that controls the amount by which the parameters are modified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01592d59",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We are now ready to train the model. We'll follow the same process to implement gradient descent:\n",
    "\n",
    "1. Generate predictions\n",
    "\n",
    "2. Calculate the loss\n",
    "\n",
    "3. Compute gradients w.r.t the weights and biases\n",
    "\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    "5. Reset the gradients to zero\n",
    "\n",
    "The only change is that we'll work batches of data instead of processing the entire training data in every iteration. Let's define a utility function `fit` that trains the model for a given number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3293c8",
   "metadata": {},
   "source": [
    "Some things to note above:\n",
    "\n",
    "* We use the data loader defined earlier to get batches of data for every iteration.\n",
    "\n",
    "* Instead of updating parameters (weights and biases) manually, we use `opt.step` to perform the update and `opt.zero_grad` to reset the gradients to zero.\n",
    "\n",
    "* We've also added a log statement that prints the loss from the last batch of data for every 10th epoch to track training progress. `loss.item` returns the actual value stored in the loss tensor.\n",
    "\n",
    "Let's train the model for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418126dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(100, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caabd792",
   "metadata": {},
   "source": [
    "Let's generate predictions using our model and verify that they're close to our targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df029c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943a57b",
   "metadata": {},
   "source": [
    "Indeed, the predictions are quite close to our targets. We have a trained a reasonably good model to predict crop yields for apples and oranges by looking at the average temperature, rainfall, and humidity in a region. We can use it to make predictions of crop yields for new regions by passing a batch containing a single row of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor([[75, 63, 44.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d0616",
   "metadata": {},
   "source": [
    "The predicted yield of apples is 54.3 tons per hectare, and that of oranges is 68.3 tons per hectare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028c221",
   "metadata": {},
   "source": [
    "# Convolutional Layer\n",
    "\n",
    "In this notebook, we visualize four filtered outputs (a.k.a. activation maps) of a convolutional layer. \n",
    "\n",
    "In this example, *we* are defining four filters that are applied to an input image by initializing the **weights** of a convolutional layer, but a trained CNN will learn the values of these weights.\n",
    "\n",
    "<img src='conv_layer.gif' height=60% width=60% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0770c",
   "metadata": {
    "id": "PkRCd9f4Jp_O"
   },
   "source": [
    "## Defining the Model (Convolutional Neural Network)\n",
    "\n",
    "In our [previous tutorial](https://jovian.ml/aakashns/04-feedforward-nn), we defined a deep neural network with fully-connected layers using `nn.Linear`. For this tutorial however, we will use a convolutional neural network, using the `nn.Conv2d` class from PyTorch.\n",
    "\n",
    "> The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel. - [Source](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" style=\"max-width:400px;\">\n",
    "\n",
    "\n",
    "Let us implement a convolution operation on a 1 channel image with a 3x3 kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f75d4d",
   "metadata": {},
   "source": [
    "For multi-channel images, a different kernel is applied to each channels, and the outputs are added together pixel-wise. \n",
    "\n",
    "Checking out the following articles to gain a better understanding of convolutions:\n",
    "\n",
    "1. [Intuitively understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) by Irhum Shafkat\n",
    "2. [Convolutions in Depth](https://sgugger.github.io/convolution-in-depth.html) by Sylvian Gugger (this article implements convolutions from scratch)\n",
    "\n",
    "There are certain advantages offered by convolutional layers when working with image data:\n",
    "\n",
    "* **Fewer parameters**: A small set of parameters (the kernel) is used to calculate outputs of the entire image, so the model has much fewer parameters compared to a fully connected layer. \n",
    "* **Sparsity of connections**: In each layer, each output element only depends on a small number of input elements, which makes the forward and backward passes more efficient.\n",
    "* **Parameter sharing and spatial invariance**: The features learned by a kernel in one part of the image can be used to detect similar pattern in a different part of another image.\n",
    "\n",
    "We will also use a [max-pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling) layers to progressively decrease the height & width of the output tensors from each convolutional layer.\n",
    "\n",
    "<img src=\"https://computersciencewiki.org/images/8/8a/MaxpoolSample2.png\" style=\"max-width:400px;\">\n",
    "\n",
    "Before we define the entire model, let's look at how a single convolutional layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ffa1d",
   "metadata": {},
   "source": [
    "### Import the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# TODO: Feel free to try out your own images here by changing img_path\n",
    "# to a file path to another image on your computer!\n",
    "img_path = 'kia.jpeg'\n",
    "\n",
    "# load color image \n",
    "bgr_img = cv2.imread(img_path)\n",
    "# convert to grayscale\n",
    "gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# normalize, rescale entries to lie in [0,1]\n",
    "gray_img = gray_img.astype(\"float32\")/255\n",
    "\n",
    "# plot image\n",
    "plt.imshow(gray_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e93fc8",
   "metadata": {},
   "source": [
    "### Define and visualize the filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## TODO: Feel free to modify the numbers here, to try out another filter!\n",
    "filter_vals = np.array([[-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1]])\n",
    "\n",
    "print('Filter shape: ', filter_vals.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining four different filters, \n",
    "# all of which are linear combinations of the `filter_vals` defined above\n",
    "\n",
    "# define four filters\n",
    "filter_1 = filter_vals\n",
    "filter_2 = -filter_1\n",
    "filter_3 = filter_1.T\n",
    "filter_4 = -filter_3\n",
    "filters = np.array([filter_1, filter_2, filter_3, filter_4])\n",
    "\n",
    "# For an example, print out the values of filter 1\n",
    "print('Filter 1: \\n', filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all four filters\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(filters[i], cmap='gray')\n",
    "    ax.set_title('Filter %s' % str(i+1))\n",
    "    width, height = filters[i].shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            ax.annotate(str(filters[i][x][y]), xy=(y,x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',\n",
    "                        color='white' if filters[i][x][y]<0 else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98999a2f",
   "metadata": {},
   "source": [
    "## Define a convolutional layer \n",
    "\n",
    "The various layers that make up any neural network are documented, [here](http://pytorch.org/docs/stable/nn.html). For a convolutional neural network, we'll start by defining a:\n",
    "* Convolutional layer\n",
    "\n",
    "Initialize a single convolutional layer so that it contains all your created filters. Note that you are not training this network; you are initializing the weights in a convolutional layer so that you can visualize what happens after a forward pass through this network!\n",
    "\n",
    "\n",
    "#### `__init__` and `forward`\n",
    "To define a neural network in PyTorch, you define the layers of a model in the function `__init__` and define the forward behavior of a network that applies those initialized layers to an input (`x`) in the function `forward`. In PyTorch we convert all inputs into the Tensor datatype, which is similar to a list data type in Python. \n",
    "\n",
    "Below, I define the structure of a class called `Net` that has a convolutional layer that can contain four 4x4 grayscale filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebda1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "# define a neural network with a single convolutional layer with four filters\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # initializes the weights of the convolutional layer to be the weights of the 4 defined filters\n",
    "        k_height, k_width = weight.shape[2:]\n",
    "        \n",
    "        # assumes there are 4 grayscale filters\n",
    "        \n",
    "        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv.weight = torch.nn.Parameter(weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # calculates the output of a convolutional layer\n",
    "        # pre- and post-activation\n",
    "        conv_x = self.conv(x)\n",
    "        activated_x = F.relu(conv_x)\n",
    "        \n",
    "        # returns both layers\n",
    "        return conv_x, activated_x\n",
    "    \n",
    "# instantiate the model and set the weights\n",
    "print(\"filter shpae\",filters.shape)\n",
    "weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)\n",
    "print(\"weight\", weight.shape)\n",
    "model = Net(weight)\n",
    "#  b x c x h x w\n",
    "# print out the layer in the network\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a44809",
   "metadata": {},
   "source": [
    "### Visualize the output of each filter\n",
    "\n",
    "First, we'll define a helper function, `viz_layer` that takes in a specific layer and number of filters (optional argument), and displays the output of that layer once an image has been passed through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e52785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for visualizing the output of a given layer\n",
    "# default number of filters is 4\n",
    "def viz_layer(layer, n_filters= 4):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    for i in range(n_filters):\n",
    "        ax = fig.add_subplot(1, n_filters, i+1, xticks=[], yticks=[])\n",
    "        # grab layer outputs\n",
    "        ax.imshow(np.squeeze(layer[0,i].data.numpy()), cmap='gray')\n",
    "        ax.set_title('Output %s' % str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a0d63",
   "metadata": {},
   "source": [
    "Let's look at the output of a convolutional layer, before and after a ReLu activation function is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot original image\n",
    "plt.imshow(gray_img, cmap='gray')\n",
    "\n",
    "# visualize all filters\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(filters[i], cmap='gray')\n",
    "    ax.set_title('Filter %s' % str(i+1))\n",
    "\n",
    "    \n",
    "# convert the image into an input Tensor\n",
    "gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "# get the convolutional layer (pre and post activation)\n",
    "conv_layer, activated_layer = model(gray_img_tensor)\n",
    "\n",
    "# visualize the output of a conv layer\n",
    "viz_layer(conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d48123",
   "metadata": {},
   "source": [
    "#### ReLu activation\n",
    "\n",
    "In this model, we've used an activation function that scales the output of the convolutional layer. We've chose a ReLu function to do this, and this function simply turns all negative pixel values in 0's (black). See the equation pictured below for input pixel values, `x`. \n",
    "\n",
    "<img src='relu_ex.png' height=50% width=50% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after a ReLu is applied\n",
    "# visualize the output of an activated conv layer\n",
    "viz_layer(activated_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eefa00",
   "metadata": {},
   "source": [
    "# CNN Example\n",
    "---\n",
    "We train a **CNN** to classify images from the CIFAR-10 database.\n",
    "\n",
    "The images in this database are small color images that fall into one of ten classes; some example images are pictured below.\n",
    "\n",
    "<img src='cifar_data.png' width=70% height=70% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a496200",
   "metadata": {},
   "source": [
    "### Test for [CUDA](http://pytorch.org/docs/stable/cuda.html)\n",
    "\n",
    "Since these are larger (32x32x3) images, it may prove useful to speed up your training time by using a GPU. CUDA is a parallel computing platform and CUDA Tensors are the same as typical Tensors, only they utilize GPU's for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058bddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f947673",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the [Data](http://pytorch.org/docs/stable/torchvision/datasets.html)\n",
    "\n",
    "Downloading may take a minute. We load in the training and test data, split the training data into a training and validation set, then create DataLoaders for each of these sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b860b",
   "metadata": {},
   "source": [
    "### Visualize a Batch of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad444fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81cd875",
   "metadata": {},
   "source": [
    "### View an Image in More Detail\n",
    "\n",
    "Here, we look at the normalized red, green, and blue (RGB) color channels as three separate, grayscale intensity images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = np.squeeze(images[3])\n",
    "channels = ['red channel', 'green channel', 'blue channel']\n",
    "\n",
    "fig = plt.figure(figsize = (36, 36)) \n",
    "for idx in np.arange(rgb_img.shape[0]):\n",
    "    ax = fig.add_subplot(1, 3, idx + 1)\n",
    "    img = rgb_img[idx]\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(channels[idx])\n",
    "    width, height = img.shape\n",
    "    thresh = img.max()/2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "            ax.annotate(str(val), xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center', size=8,\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0011a54",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the Network [Architecture](http://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "This time, you'll define a CNN architecture. Instead of an MLP, which used linear, fully-connected layers, you'll use the following:\n",
    "* [Convolutional layers](https://pytorch.org/docs/stable/nn.html#conv2d), which can be thought of as stack of filtered images.\n",
    "* [Maxpooling layers](https://pytorch.org/docs/stable/nn.html#maxpool2d), which reduce the x-y size of an input, keeping only the most _active_ pixels from the previous layer.\n",
    "* The usual Linear + Dropout layers to avoid overfitting and produce a 10-dim output.\n",
    "\n",
    "A network with 2 convolutional layers is shown in the image below and in the code, and you've been given starter code with one convolutional and one maxpooling layer.\n",
    "\n",
    "<img src='2_layer_conv.png' height=50% width=50% />\n",
    "\n",
    "#### TODO: Define a model with multiple convolutional layers, and define the feedforward metwork behavior.\n",
    "\n",
    "The more convolutional layers you include, the more complex patterns in color and shape a model can detect. It's suggested that your final model include 2 or 3 convolutional layers as well as linear layers + dropout in between to avoid overfitting. \n",
    "\n",
    "It's good practice to look at existing research and implementations of related models as a starting point for defining your own models. You may find it useful to look at [this PyTorch classification example](https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py) or [this, more complex Keras example](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py) to help decide on a final structure.\n",
    "\n",
    "#### Output volume for a convolutional layer\n",
    "\n",
    "To compute the output size of a given convolutional layer we can perform the following calculation (taken from [Stanford's cs231n course](http://cs231n.github.io/convolutional-networks/#layers)):\n",
    "> We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by `(W−F+2P)/S +1`. \n",
    "\n",
    "For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer (sees 32x32x3 image tensor)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        # convolutional layer (sees 16x16x16 tensor)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        # convolutional layer (sees 8x8x32 tensor)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (64 * 4 * 4 -> 500)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n",
    "        # linear layer (500 -> 10)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37246d96",
   "metadata": {},
   "source": [
    "### Specify [Loss Function](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "Decide on a loss and optimization function that is best suited for this classification task. The linked code examples from above, may be a good starting point; [this PyTorch classification example](https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py) or [this, more complex Keras example](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py). Pay close attention to the value for **learning rate** as this value determines how your model converges to a small error.\n",
    "\n",
    "#### TODO: Define the loss and optimizer and see how these choices change the loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa919d2",
   "metadata": {},
   "source": [
    "---\n",
    "## Train the Network\n",
    "\n",
    "Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting. (In fact, in the below example, we could have stopped around epoch 33 or so!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        optimizer.zero_grad()\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04a08e",
   "metadata": {},
   "source": [
    "###  Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e26fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_cifar.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6016c6",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network\n",
    "\n",
    "Test your trained model on previously unseen data! A \"good\" result will be a CNN that gets around 70% (or more, try your best!) accuracy on these test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba21e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for data, target in test_loader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116a4ab",
   "metadata": {},
   "source": [
    "### Question: What are your model's weaknesses and how might they be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b84480",
   "metadata": {},
   "source": [
    "**Answer**: This model seems to do best on vehicles rather than animals. For example, it does best on the automobile class and worst on the cat class. I suspect it's because animals vary in color and size and so it would improve this model if I could increase the number of animal images in the first place or perhaps if I added another convolutional layer to detect finer patterns in these images. I could also experiment with a smaller learning rate so that the model takes small steps in the right direction as it is training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752308ce",
   "metadata": {},
   "source": [
    "### Visualize Sample Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f938e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images.numpy()\n",
    "\n",
    "# move model inputs to cuda, if GPU available\n",
    "if train_on_gpu:\n",
    "    images = images.cuda()\n",
    "\n",
    "# get sample outputs\n",
    "output = model(images)\n",
    "# convert output probabilities to predicted class\n",
    "_, preds_tensor = torch.max(output, 1)\n",
    "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx] if not train_on_gpu else images[idx].cpu())\n",
    "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
    "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb0e859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
